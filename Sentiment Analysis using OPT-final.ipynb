{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8806003d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0c3b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: filelock in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.9.0\n",
      "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.22.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\HADOOP\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\HADOOP\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b176804",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.5.1-py3-none-any.whl (431 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in c:\\users\\hadoop\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (0.9.1)\n",
      "Collecting dill<0.3.6\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2022.2.0)\n",
      "Requirement already satisfied: packaging in d:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: aiohttp in d:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (4.64.0)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2.27.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-9.0.0-cp39-cp39-win_amd64.whl (19.6 MB)\n",
      "Requirement already satisfied: pandas in d:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (1.4.2)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: filelock in d:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: dill, xxhash, responses, pyarrow, multiprocess, datasets\n",
      "Successfully installed datasets-2.5.1 dill-0.3.5.1 multiprocess-0.70.13 pyarrow-9.0.0 responses-0.18.0 xxhash-3.0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script plasma_store.exe is installed in 'C:\\Users\\HADOOP\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script datasets-cli.exe is installed in 'C:\\Users\\HADOOP\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "#pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0c5cbc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n",
      "Collecting torch==1.10.1+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torch-1.10.1%2Bcu113-cp39-cp39-win_amd64.whl (2442.3 MB)\n",
      "Collecting torchvision==0.11.2+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.11.2%2Bcu113-cp39-cp39-win_amd64.whl (3.2 MB)\n",
      "Collecting torchaudio==0.10.1+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.10.1%2Bcu113-cp39-cp39-win_amd64.whl (336 kB)\n",
      "Requirement already satisfied: typing-extensions in d:\\programdata\\anaconda3\\lib\\site-packages (from torch==1.10.1+cu113) (4.1.1)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from torchvision==0.11.2+cu113) (9.0.1)\n",
      "Requirement already satisfied: numpy in d:\\programdata\\anaconda3\\lib\\site-packages (from torchvision==0.11.2+cu113) (1.21.5)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.10.1+cu113 torchaudio-0.10.1+cu113 torchvision-0.11.2+cu113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\HADOOP\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e198b4",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "786eeb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95026e76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (C:/Users/HADOOP/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc810f24193d4c63815ef7442b2cae9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad19b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c61a3",
   "metadata": {},
   "source": [
    " - Create a smaller dataset for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff13cbdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:/Users/HADOOP/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1\\cache-8a9e43a6ac4acdff.arrow\n",
      "Loading cached shuffled indices for dataset at C:/Users/HADOOP/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1\\cache-2eff9f118d84c6fe.arrow\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select([i for i in list(range(3000))])\n",
    "small_test_dataset = dataset[\"test\"].shuffle(seed=42).select([i for i in list(range(300))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e40b1",
   "metadata": {},
   "source": [
    " - We will be using Facebook's opt-350m tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cf746a",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b9e3716",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b05442",
   "metadata": {},
   "source": [
    " - Preparing the input to our model using the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f97e62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'small_train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_function\u001b[39m(examples):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m tokenized_train \u001b[38;5;241m=\u001b[39m \u001b[43msmall_train_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mmap(preprocess_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m tokenized_test \u001b[38;5;241m=\u001b[39m small_test_dataset\u001b[38;5;241m.\u001b[39mmap(preprocess_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'small_train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    " \n",
    "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = small_test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "435973c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quadro RTX 8000'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b0b5f",
   "metadata": {},
   "source": [
    "# Model Training using Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2137eb42",
   "metadata": {},
   "source": [
    " - We can now use data collator to speed up training time by converting our training samples to PyTorch tensors and concatenate them with the correct amount of padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1213e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de784076",
   "metadata": {},
   "source": [
    " - We will be throwing away the pretraining head of the OPT-350m fine tuned model and replacing it with a classification head fine-tuned for sentiment analysis. We perform transfer learning because the original model is not trained for sentiment analysis. This is basically transfer learning from the pretrained model to our own sentiment analysis model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e138ba6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"Tianyi98/opt-350m-finetuned-cola\", \n",
    "                                                           num_labels=2,\n",
    "                                                           id2label={0: 'Negative', 1: 'Positive'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e7f52",
   "metadata": {},
   "source": [
    " - We define a function to get the metrics to evaluate our model. The metrics we will use are accuracy and f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bab1feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    load_accuracy = load_metric(\"accuracy\")\n",
    "    load_f1 = load_metric(\"f1\")\n",
    "  \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aff657",
   "metadata": {},
   "source": [
    " - Here, we will declare all the hyperparameters and use the Trainer API to train our models and push it to our repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa639ac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HADOOP\\Desktop\\finetuning-sentiment-model-3000-samples is already a clone of https://huggingface.co/DravenTay/finetuning-sentiment-model-3000-samples. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    " \n",
    "repo_name = \"finetuning-sentiment-model-3000-samples\"\n",
    "#device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    " \n",
    "# Hyperparameters we can tune\n",
    "training_args = TrainingArguments(\n",
    "   output_dir=repo_name,\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=4,\n",
    "   per_device_eval_batch_size=4,\n",
    "   num_train_epochs=2,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   push_to_hub=True,\n",
    "   # Login to your Hugging Face and get the Access Token\n",
    "   hub_token = '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'\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1d173",
   "metadata": {},
   "source": [
    "Explanation : We use a batch size of 4 so it does not require much memory allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d1a9107",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `OPTForSequenceClassification.forward` and have been ignored: text. If text are not expected by `OPTForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\HADOOP\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 15:42, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.598200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.329400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.212900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to finetuning-sentiment-model-3000-samples\\checkpoint-750\n",
      "Configuration saved in finetuning-sentiment-model-3000-samples\\checkpoint-750\\config.json\n",
      "Model weights saved in finetuning-sentiment-model-3000-samples\\checkpoint-750\\pytorch_model.bin\n",
      "tokenizer config file saved in finetuning-sentiment-model-3000-samples\\checkpoint-750\\tokenizer_config.json\n",
      "Special tokens file saved in finetuning-sentiment-model-3000-samples\\checkpoint-750\\special_tokens_map.json\n",
      "tokenizer config file saved in finetuning-sentiment-model-3000-samples\\tokenizer_config.json\n",
      "Special tokens file saved in finetuning-sentiment-model-3000-samples\\special_tokens_map.json\n",
      "Saving model checkpoint to finetuning-sentiment-model-3000-samples\\checkpoint-1500\n",
      "Configuration saved in finetuning-sentiment-model-3000-samples\\checkpoint-1500\\config.json\n",
      "Model weights saved in finetuning-sentiment-model-3000-samples\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in finetuning-sentiment-model-3000-samples\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in finetuning-sentiment-model-3000-samples\\checkpoint-1500\\special_tokens_map.json\n",
      "tokenizer config file saved in finetuning-sentiment-model-3000-samples\\tokenizer_config.json\n",
      "Special tokens file saved in finetuning-sentiment-model-3000-samples\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.380165761311849, metrics={'train_runtime': 944.695, 'train_samples_per_second': 6.351, 'train_steps_per_second': 1.588, 'total_flos': 5848613018099712.0, 'train_loss': 0.380165761311849, 'epoch': 2.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceaedb2",
   "metadata": {},
   "source": [
    "Explanation : The training loss decreases from epoch 1 to epoch 2, we can conclude that it was training successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "935f0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clear cache\n",
    "#torch.cuda.empty_cache()\n",
    "# To check usage\n",
    "#torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe585c",
   "metadata": {},
   "source": [
    " - Evaluate Training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1fec8cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `OPTForSequenceClassification.forward` and have been ignored: text. If text are not expected by `OPTForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 300\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HADOOP\\AppData\\Local\\Temp\\ipykernel_111568\\841120470.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  load_accuracy = load_metric(\"accuracy\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390b0ed4b0b04e5398f96573f400cf9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fc69ce49e1452eba6d690074e7eb10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.41327524185180664,\n",
       " 'eval_accuracy': 0.92,\n",
       " 'eval_f1': 0.9205298013245033,\n",
       " 'eval_runtime': 15.2123,\n",
       " 'eval_samples_per_second': 19.721,\n",
       " 'eval_steps_per_second': 4.93,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate training results\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821acde",
   "metadata": {},
   "source": [
    "Explanation : With only 2 epochs and 3000 training samples, our transfer learning model was able to achieve 92% accuracy and 0.92 f1 score on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb0248",
   "metadata": {},
   "source": [
    "# Save and Load the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea5ff4c",
   "metadata": {},
   "source": [
    " - Pushing our trained model to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d093df01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to finetuning-sentiment-model-3000-samples\n",
      "Configuration saved in finetuning-sentiment-model-3000-samples\\config.json\n",
      "Model weights saved in finetuning-sentiment-model-3000-samples\\pytorch_model.bin\n",
      "tokenizer config file saved in finetuning-sentiment-model-3000-samples\\tokenizer_config.json\n",
      "Special tokens file saved in finetuning-sentiment-model-3000-samples\\special_tokens_map.json\n",
      "To https://huggingface.co/DravenTay/finetuning-sentiment-model-3000-samples\n",
      "   0639d23..5bb8100  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9b914e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc42016a85d45a585258fc256ca75ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/849 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cefa02b9273d42e6b0462b8bcfcb1732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/999k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ca3478b573430a952d9a476d50a21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d8a22543b24a8c805768eef5bb2033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/548 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff59bd61f0045dbb04a027198bbbd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.89k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the pipeline from my hub\n",
    "from transformers import pipeline\n",
    "\n",
    "# Requires developer mode\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DravenTay/finetuning-sentiment-model-3000-samples\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"DravenTay/finetuning-sentiment-model-3000-samples\")\n",
    "sentiment_model1 = pipeline(model=\"DravenTay/finetuning-sentiment-model-3000-samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8da7684",
   "metadata": {},
   "source": [
    "# Web Scraping and Sentiment Analysis on Scraped Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06aa84",
   "metadata": {},
   "source": [
    " - Web Scraper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d07f8f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=9347be866fb9810c0a8d6810c5fb8061606dce06cae284f9104516687470f4e3\n",
      "  Stored in directory: c:\\users\\hadoop\\appdata\\local\\pip\\cache\\wheels\\73\\2b\\cb\\099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install bs4\n",
    "#pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b4af924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver as wd\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import urllib.parse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76ed580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter got Top, Latest, People, Photo, Video search option, In our case we will be only choose [Top, Latest]\n",
    "postType = [\"top\",\"live\"]\n",
    "\n",
    "# Twitter Explorer Link Format. q is for Keywords, f is for postType\n",
    "twitterLink = \"https://twitter.com/search?q={}&src=typed_query&f={}\"\n",
    "\n",
    "# Get current file directory\n",
    "curdir = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3daac3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnyEc:\n",
    "    \"\"\" Use with WebDriverWait to combine expected_conditions\n",
    "        in an OR.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.ecs = args\n",
    "    def __call__(self, driver):\n",
    "        for fn in self.ecs:\n",
    "            try:\n",
    "                res = fn(driver)\n",
    "                if res:\n",
    "                    return True\n",
    "                    # Or return res if you need the element found\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d22edb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                    - Close browser          - Comments num\n",
    "#                                    after run                to collect\n",
    "def searchTopic(topic, latest=False, close=False, timeout=10, comments=10):\n",
    "    # Open Google Chrome Browser\n",
    "    driver = wd.Chrome(executable_path=(str(r'{}\\chromedriver_win32\\chromedriver.exe').format(curdir)))\n",
    "    encodeTopic = urllib.parse.quote(topic, safe='')\n",
    "    driver.get(twitterLink.format(encodeTopic, postType[latest]))\n",
    "    try:\n",
    "        WebDriverWait(driver, timeout).until(AnyEc(\n",
    "            EC.presence_of_element_located(\n",
    "                 (By.CSS_SELECTOR, \"div[data-testid='empty_state_header_text']\")),\n",
    "            EC.presence_of_element_located(\n",
    "                 (By.TAG_NAME, 'article'))\n",
    "        ))\n",
    "        if(len(driver.find_elements(By.TAG_NAME, 'article'))==0):\n",
    "            print(\"No result\")\n",
    "        else:\n",
    "            totalComments = 0;\n",
    "            commentArray = [];\n",
    "            pageTitle = [];\n",
    "            time.sleep(3)\n",
    "            screen_height = driver.execute_script(\"return window.screen.height;\")\n",
    "            t1 = 1\n",
    "            with open('comments\\{}.csv'.format(topic), 'w', newline='', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                while True:\n",
    "                    html = driver.page_source;\n",
    "                    element = BeautifulSoup(html, \"html.parser\")\n",
    "                    t1+=1\n",
    "                    for row in element.find_all('div', {'class':'css-1dbjc4n r-1iusvr4 r-16y2uox r-1777fci r-kzbkwu'}):\n",
    "                        for i in row.find_all('a', {'role':\"link\", 'dir':\"auto\"}):\n",
    "                            driver.execute_script(\"window.open('');\")\n",
    "                            driver.switch_to.window(driver.window_handles[-1])\n",
    "                            driver.get('https://twitter.com'+i.get('href'))\n",
    "                            time.sleep(3)\n",
    "                            # prevent duplicate post\n",
    "                            if(driver.title in pageTitle):\n",
    "                                driver.close()\n",
    "                                driver.switch_to.window(driver.window_handles[0])\n",
    "                                continue\n",
    "                            else:\n",
    "                                pageTitle.append(driver.title)\n",
    "                            t=1\n",
    "                            while True:\n",
    "                                driver.execute_script(\"window.scrollTo(0, {screen_height}*{t});\".format(screen_height=screen_height, t=t))  \n",
    "                                t += 1\n",
    "                                time.sleep(1)\n",
    "                                scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "                                htmlInner = driver.execute_script(\"return document.body.innerHTML;\")\n",
    "                                elementInner = BeautifulSoup(htmlInner, \"html.parser\")\n",
    "                                for y in elementInner.find_all('article', attrs={'tabindex':\"0\", \"role\":\"article\"}):\n",
    "                                    j = y.find('div', attrs={'dir':'auto', \"data-testid\":\"tweetText\"})\n",
    "                                    #print(j.text)\n",
    "                                    if(j.text not in commentArray):\n",
    "                                        if(j.text != \"\"):\n",
    "                                            commentArray.append(j.text)\n",
    "                                            writer.writerow([j.text])\n",
    "                                    if(len(commentArray) >= comments):\n",
    "                                        if close:\n",
    "                                            driver.close()\n",
    "                                            driver.switch_to.window(driver.window_handles[0])\n",
    "                                            driver.close()\n",
    "                                        os._exit(os.EX_OK)\n",
    "                                if((screen_height * t) > scroll_height):\n",
    "                                    break\n",
    "                            driver.close()\n",
    "                            driver.switch_to.window(driver.window_handles[0])\n",
    "                    driver.execute_script(\"window.scrollTo(0, {screen_height}*{t});\".format(screen_height=screen_height, t=t1))\n",
    "                    time.sleep(1)\n",
    "                    scroll_height1 = driver.execute_script(\"return document.body.scrollHeight;\") \n",
    "                    if((screen_height * t) > scroll_height1):\n",
    "                        driver.close()\n",
    "                        break\n",
    "    except Exception as E:\n",
    "        print(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "176bbdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_8772\\4060671249.py:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wd.Chrome(executable_path=(str(r'{}\\chromedriver_win32\\chromedriver.exe').format(curdir)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module 'os' has no attribute 'EX_OK'\n"
     ]
    }
   ],
   "source": [
    "# Search topic pass in topic, and number of comments\n",
    "searchTopic(\"bitcoin\", close=False, comments=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd7529cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HADOOP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be705c5",
   "metadata": {},
   "source": [
    " - Load the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edcaeacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good morning. This was the most-read &amp; most-em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Crypto morning nuggets (1/3): \\n\\nSurprise! W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Crypto morning nuggets (2/3): \\n\\n@INTERPOL_H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liz Truss keeping her promise to hit the ground.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Excellent.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Good morning. This was the most-read & most-em...\n",
       "1  #Crypto morning nuggets (1/3): \\n\\nSurprise! W...\n",
       "2  #Crypto morning nuggets (2/3): \\n\\n@INTERPOL_H...\n",
       "3   Liz Truss keeping her promise to hit the ground.\n",
       "4                                         Excellent."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "data = pd.read_csv(\"bitcoin.csv\", names=[\"text\"], header=None, encoding_errors= 'replace')\n",
    "data.head()\n",
    "#stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5deba896",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good morning. This was the most-read &amp; most-em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Crypto morning nuggets (1/3): \\n\\nSurprise! W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Crypto morning nuggets (2/3): \\n\\n@INTERPOL_H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liz Truss keeping her promise to hit the ground.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Excellent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>They are acting like a bunch of students that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adding to inflation!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Big question:\\nWhy should people vote for @lab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Because no matter what they do it's going to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What's your solution?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Good morning. This was the most-read & most-em...\n",
       "1  #Crypto morning nuggets (1/3): \\n\\nSurprise! W...\n",
       "2  #Crypto morning nuggets (2/3): \\n\\n@INTERPOL_H...\n",
       "3   Liz Truss keeping her promise to hit the ground.\n",
       "4                                         Excellent.\n",
       "5  They are acting like a bunch of students that ...\n",
       "6                              Adding to inflation!!\n",
       "7  Big question:\\nWhy should people vote for @lab...\n",
       "8  Because no matter what they do it's going to b...\n",
       "9                              What's your solution?"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c54dbd",
   "metadata": {},
   "source": [
    "# Comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d50919c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning. This was the most-read & most-emailed story on @TheTerminal during the past 24 hours as of 8 am in NY:\n",
      "\n",
      "In Truss the markets don't trust -- the pound is getting hammered, approaching parity against the  while some call for BOE action\n",
      "{'label': 'Negative', 'score': 0.5971994996070862}\n",
      "\n",
      "\n",
      "\n",
      "#Crypto morning nuggets (1/3): \n",
      "\n",
      "Surprise! Wild swings in markets have left cryptocurrencies like #Bitcoin dueling with the dollar for the title of top-performing asset this quarter  https://bloomberg.com/news/articles/2022-09-26/market-churn-leaves-crypto-dueling-dollar-as-quarter-s-top-asset?sref=qpwbmgSU� via @markets @crytpo\n",
      "\n",
      "I think the big boys algos will amaze you like they did on friday close.\n",
      "{'label': 'Negative', 'score': 0.5518010258674622}\n",
      "\n",
      "\n",
      "\n",
      "#Crypto morning nuggets (2/3): \n",
      "\n",
      "@INTERPOL_HQ has issued a red notice to locate and arrest fugitive Terraform Labs co-founder Do Kwon (@stablekwon), South Korea says  https://bloomberg.com/news/articles/2022-09-26/south-korea-says-interpol-issued-red-notice-for-terra-s-do-kwon?sref=qpwbmgSU� via @technology @crypto\n",
      "I think Before free fall we might touch Upper line\n",
      "{'label': 'Negative', 'score': 0.9817901849746704}\n",
      "\n",
      "\n",
      "\n",
      "Liz Truss keeping her promise to hit the ground.\n",
      "{'label': 'Positive', 'score': 0.9033911824226379}\n",
      "\n",
      "\n",
      "\n",
      "Excellent.\n",
      "{'label': 'Positive', 'score': 0.9983015060424805}\n",
      "\n",
      "\n",
      "\n",
      "They are acting like a bunch of students that have just been told they are not getting the house deposit back\n",
      "{'label': 'Negative', 'score': 0.959954023361206}\n",
      "\n",
      "\n",
      "\n",
      "Adding to inflation!!\n",
      "{'label': 'Positive', 'score': 0.9171885251998901}\n",
      "\n",
      "\n",
      "\n",
      "Big question:\n",
      "Why should people vote for @labour at next General Election?\n",
      "{'label': 'Negative', 'score': 0.9704145789146423}\n",
      "\n",
      "\n",
      "\n",
      "Because no matter what they do it's going to be better than the people who caused our economy to do whatever the hell it's doing today.\n",
      "{'label': 'Positive', 'score': 0.9921954870223999}\n",
      "\n",
      "\n",
      "\n",
      "What's your solution?\n",
      "{'label': 'Positive', 'score': 0.953190803527832}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Output of our own model\n",
    "for i in range(len(data)):\n",
    "    print(data[\"text\"][i])\n",
    "    prediction = sentiment_model1(data[\"text\"][i])\n",
    "    pred_dict = prediction[0]\n",
    "    if pred_dict['label'] == 'LABEL_0':\n",
    "        pred_dict['label'] = 'Negative'\n",
    "    else:\n",
    "        pred_dict['label'] = 'Positive'\n",
    "    print(pred_dict)\n",
    "    print(\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb6eac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load another model to compare \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Tianyi98/opt-350m-finetuned-cola\", \n",
    "                                                           num_labels=2,\n",
    "                                                           id2label={0: 'Negative', 1: 'Positive'})\n",
    "sentiment_model2 = pipeline(\"sentiment-analysis\", tokenizer=tokenizer, model=model, device = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2ba2faf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning. This was the most-read & most-emailed story on @TheTerminal during the past 24 hours as of 8 am in NY:\n",
      "\n",
      "In Truss the markets don't trust -- the pound is getting hammered, approaching parity against the  while some call for BOE action\n",
      "[{'label': 'Positive', 'score': 0.9438297748565674}]\n",
      "\n",
      "\n",
      "\n",
      "#Crypto morning nuggets (1/3): \n",
      "\n",
      "Surprise! Wild swings in markets have left cryptocurrencies like #Bitcoin dueling with the dollar for the title of top-performing asset this quarter  https://bloomberg.com/news/articles/2022-09-26/market-churn-leaves-crypto-dueling-dollar-as-quarter-s-top-asset?sref=qpwbmgSU� via @markets @crytpo\n",
      "\n",
      "I think the big boys algos will amaze you like they did on friday close.\n",
      "[{'label': 'Negative', 'score': 0.5801198482513428}]\n",
      "\n",
      "\n",
      "\n",
      "#Crypto morning nuggets (2/3): \n",
      "\n",
      "@INTERPOL_HQ has issued a red notice to locate and arrest fugitive Terraform Labs co-founder Do Kwon (@stablekwon), South Korea says  https://bloomberg.com/news/articles/2022-09-26/south-korea-says-interpol-issued-red-notice-for-terra-s-do-kwon?sref=qpwbmgSU� via @technology @crypto\n",
      "I think Before free fall we might touch Upper line\n",
      "[{'label': 'Positive', 'score': 0.7761542201042175}]\n",
      "\n",
      "\n",
      "\n",
      "Liz Truss keeping her promise to hit the ground.\n",
      "[{'label': 'Positive', 'score': 0.6016188263893127}]\n",
      "\n",
      "\n",
      "\n",
      "Excellent.\n",
      "[{'label': 'Positive', 'score': 0.9050458073616028}]\n",
      "\n",
      "\n",
      "\n",
      "They are acting like a bunch of students that have just been told they are not getting the house deposit back\n",
      "[{'label': 'Positive', 'score': 0.9961177110671997}]\n",
      "\n",
      "\n",
      "\n",
      "Adding to inflation!!\n",
      "[{'label': 'Positive', 'score': 0.6179782748222351}]\n",
      "\n",
      "\n",
      "\n",
      "Big question:\n",
      "Why should people vote for @labour at next General Election?\n",
      "[{'label': 'Positive', 'score': 0.9987338185310364}]\n",
      "\n",
      "\n",
      "\n",
      "Because no matter what they do it's going to be better than the people who caused our economy to do whatever the hell it's doing today.\n",
      "[{'label': 'Positive', 'score': 0.9847918748855591}]\n",
      "\n",
      "\n",
      "\n",
      "What's your solution?\n",
      "[{'label': 'Positive', 'score': 0.9986170530319214}]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\opt1\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Output of Transformer's pretrained model\n",
    "for i in range(len(data)):\n",
    "    print(data[\"text\"][i])\n",
    "    print(sentiment_model2(data[\"text\"][i]))\n",
    "    print(\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd63faa",
   "metadata": {},
   "source": [
    "# User Interface (Deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7eebefc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def fun_sentiment_analysis():\n",
    "    while True :\n",
    "        print(\"\\n\\n----------------------------------------------------------\")\n",
    "        print(\"                         Sentiment Analysis \")\n",
    "        print(\"----------------------------------------------------------\")\n",
    "        print(\" Select your model :\")\n",
    "        print(\" 1. Our own trained OPT 350m model\")\n",
    "        print(\" 2. Tian Yi's pretrained OPT 350m model\")\n",
    "        print(\"----------------------------------------------------------\")\n",
    "        choice = input(\"Enter your choice : \")\n",
    "        if choice < '1' or choice > '2':\n",
    "            print(\"Invalid input, please enter a number between 1 - 2\")\n",
    "            time.sleep(2)\n",
    "        else :\n",
    "            sentence = input(\"Enter yor sentence here : \")\n",
    "            if choice == '1':\n",
    "                pred = sentiment_model1(sentence)\n",
    "                pred_dict = pred[0]\n",
    "                if pred_dict['label'] == 'LABEL_0':\n",
    "                    prediction = 'Negative'\n",
    "                else:\n",
    "                    prediction = 'Positive'\n",
    "            elif choice == '2' :\n",
    "                pred = sentiment_model2(sentence)\n",
    "                prediction = pred [0]\n",
    "                prediction = prediction['label']\n",
    "            print(\"\\n\\nResult : \")\n",
    "            print(\"----------------------\")\n",
    "            print(\"Your sentence is a \", prediction, \"sentence.\")\n",
    "            time.sleep(2)\n",
    "            another = input(\"Try again? (y = yes, else = no) : \")\n",
    "            if another != 'y':\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "302e20ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_sentiment_scraped() :\n",
    "    print(\"\\n\\n----------------------------------------------------------\")\n",
    "    print(\"      Sentiment Analysis on Scraped Comments \")\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    print(\" Select your model :\")\n",
    "    print(\" 1. Our own trained OPT 350m model\")\n",
    "    print(\" 2. Tian Yi's pretrained OPT 350m model\")\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    choice = input(\"Enter your choice : \")\n",
    "    while True :\n",
    "        if choice < '1' or choice > '2':\n",
    "            print(\"Invalid input, please enter a number between 1 - 2\")\n",
    "            time.sleep(2)\n",
    "            choice = input(\"Enter your choice : \")\n",
    "        else :\n",
    "            if choice == '1':\n",
    "                model = sentiment_model1\n",
    "            elif choice == '2':\n",
    "                model = sentiment_model2\n",
    "            break\n",
    "    \n",
    "    file = input(\"Enter your csv file stored inside the folder 'comments' (with exetension .csv) : \")\n",
    "    file = \"comments\\\\\" + file\n",
    "    user_data = pd.read_csv(file, names=[\"text\"], header=None, encoding_errors= 'replace')\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"RESULTS :\")\n",
    "    print(\"----------------------\")\n",
    "    for i in range(len(user_data)):\n",
    "        print(user_data[\"text\"][i])\n",
    "        prediction = model(user_data[\"text\"][i])\n",
    "        pred_dict = prediction[0]\n",
    "        if pred_dict['label'] == 'LABEL_0':\n",
    "            pred_dict['label'] = 'Negative'\n",
    "        elif pred_dict['label'] == 'LABEL_1':\n",
    "            pred_dict['label'] = 'Positive'\n",
    "        print(pred_dict)\n",
    "        print(\"\\n\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "974b6b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_twitter_scraping():\n",
    "    print(\"\\n\\n----------------------------------------------------------\")\n",
    "    print(\"                         Twitter Scraping \")\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    topic = input(\" Enter your topic : \")\n",
    "    while True : \n",
    "        n_comments = input(\"Enter number of comments from this topic (1 to  20) : \")\n",
    "        if n_comments <'1' or n_comments > '20' :\n",
    "            print(\"Invalid input, please try again !\")\n",
    "            time.sleep(2)\n",
    "        else :\n",
    "            n_comments = int(n_comments)\n",
    "            break\n",
    "    # Search topic pass in topic, and number of comments\n",
    "    searchTopic(topic, close=False, comments=n_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cd93cb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------------------------\n",
      "                 NoidAI's Sentiment Analyzer\n",
      "----------------------------------------------------------\n",
      " Functions : \n",
      " 1. Sentiment Analysis on a sentence\n",
      " 2. Sentiment Analysis on a scraped csv\n",
      " 3. Scrape Twitter for comments\n",
      " 4 Exit\n",
      "----------------------------------------------------------\n",
      "Enter your choice : 3\n",
      "\n",
      "\n",
      "----------------------------------------------------------\n",
      "                         Twitter Scraping \n",
      "----------------------------------------------------------\n",
      " Enter your topic : gold\n",
      "Enter number of comments from this topic (1 to  20) : 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_8772\\4060671249.py:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wd.Chrome(executable_path=(str(r'{}\\chromedriver_win32\\chromedriver.exe').format(curdir)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module 'os' has no attribute 'EX_OK'\n",
      "\n",
      "\n",
      "----------------------------------------------------------\n",
      "                 NoidAI's Sentiment Analyzer\n",
      "----------------------------------------------------------\n",
      " Functions : \n",
      " 1. Sentiment Analysis on a sentence\n",
      " 2. Sentiment Analysis on a scraped csv\n",
      " 3. Scrape Twitter for comments\n",
      " 4 Exit\n",
      "----------------------------------------------------------\n",
      "Enter your choice : 4\n",
      "Bye, have a nice day :)\n"
     ]
    }
   ],
   "source": [
    "def main() :\n",
    "    while True :\n",
    "        print(\"\\n\\n----------------------------------------------------------\")\n",
    "        print(\"                 NoidAI's Sentiment Analyzer\")\n",
    "        print(\"----------------------------------------------------------\")\n",
    "        print(\" Functions : \")\n",
    "        print(\" 1. Sentiment Analysis on a sentence\")\n",
    "        print(\" 2. Sentiment Analysis on a scraped csv\")\n",
    "        print(\" 3. Scrape Twitter for comments\")\n",
    "        print(\" 4 Exit\")\n",
    "        print(\"----------------------------------------------------------\")\n",
    "\n",
    "        choice = input(\"Enter your choice : \")\n",
    "\n",
    "        if choice < '1' or choice > '4':\n",
    "            print(\"Invalid input, please enter a number between 1 - 4\")\n",
    "            time.sleep(2)\n",
    "        else :\n",
    "            if choice == '1':\n",
    "                fun_sentiment_analysis()\n",
    "            elif choice == '2':\n",
    "                fun_sentiment_scraped()\n",
    "            elif choice == '3' :\n",
    "                fun_twitter_scraping()\n",
    "            else :\n",
    "                print(\"Bye, have a nice day :)\")\n",
    "                \n",
    "                break\n",
    "                \n",
    "\n",
    "\n",
    "# Call the main function\n",
    "main()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b3f23",
   "metadata": {},
   "source": [
    "---------------------------------------- END -------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OPT",
   "language": "python",
   "name": "opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
